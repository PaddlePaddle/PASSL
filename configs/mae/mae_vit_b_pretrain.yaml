epochs: 800
output_dir: output_dir

model:
  name: MAE_PRETRAIN
  architecture:
      name: MAE
      patch_size: 16
      embed_dim: 768
      depth: 12
      num_heads: 12
      decoder_embed_dim: 512
      decoder_depth: 8
      decoder_num_heads: 16
      mlp_ratio: 4


dataloader:
  train:
    num_workers: 8
    sampler:
      batch_size: 128
      shuffle: true
      drop_last: True
    dataset:
      name: ImageNet
      dataroot: data/ILSVRC2012/train/
      return_label: True
      transforms:
        - name: ToRGB
        - name: RandomResizedCrop
          size: 224
          scale: [0.75, 1.]
          ratio: [1., 1.]
          interpolation: 'bicubic'
        - name: Transpose
        - name: NormalizeImage
          scale: 1.0/255.0
          mean: [0.485, 0.456, 0.406]
          std: [0.229, 0.224, 0.225]

lr_scheduler:
  name: LinearWarmup
  learning_rate:
    name: CosineAnnealingDecay
    learning_rate: 3.75e-5  # 8 gpus
    T_max: 800
    eta_min: 1e-5
  warmup_steps: 40
  start_lr: 1e-6
  end_lr: 0.001

optimizer:
  name: AdamW
  beta1: 0.9
  beta2: 0.95
  weight_decay: 0.05


log_config:
    name: LogHook
    interval: 10
