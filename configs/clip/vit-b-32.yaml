epochs: 10
output_dir: output_dir

model:
  name: CLIPWrapper
  architecture:
    name: CLIP
    embed_dim: 512
    image_resolution: 224
    vision_layers: 12
    vision_width: 768
    vision_patch_size: 32
    context_length: 77
    vocab_size: 49408
    transformer_width: 512
    transformer_heads: 8
    transformer_layers: 12
    qkv_bias: True
    pre_norm: True
    proj: True
    patch_bias: False
  head:
    name: CLIPHead

dataloader:
  train:
    num_workers: 8
    sampler:
      batch_size: 128
      shuffle: true
      drop_last: True
    dataset:
      name: TextImageDataset
      dataroot: data/FOOD101/captions/
      transforms:
        - name: ToRGB
        - name: RandomResizedCrop
          size: 224
          scale: [0.75, 1.]
          ratio: [1., 1.]
        - name: NormalizeImage                                                                                                                                                                                                                                                 
          scale: 1.0/255.0
          mean: [0.485, 0.456, 0.406]
          std: [0.229, 0.224, 0.225]
      shuffle: False

solver:
  lr:
    name: CosineAnnealingDecay
    learning_rate: 0.00001
    T_max: 10

optimizer:
  name: AdamW
  beta1: 0.9
  beta2: 0.98
  epsilon: 1e-8
  weight_decay: 0.0005

optimizer_config:
  name: CLIPOptimizerHook

log_config:
    name: LogHook
    interval: 10
