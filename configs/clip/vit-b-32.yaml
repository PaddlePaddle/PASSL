epochs: 10
output_dir: output_dir

model:
  name: CLIPWrapper
  architecture:
    name: CLIP
    embed_dim: 512
    image_resolution: 224
    vision_layers: 12
    vision_width: 768
    vision_patch_size: 32
    context_length: 77
    vocab_size: 49408
    transformer_width: 512
    transformer_heads: 8
    transformer_layers: 12
    qkv_bias: True
  head:
    name: CLIPHead

dataloader:
  train:
    num_workers: 0
    sampler:
      batch_size: 128
      shuffle: true
      drop_last: True
    dataset:
      name: TextImageDataset
      dataroot: data/FOOD101/captions/
      transforms:
        - name: ToRGB
        - name: RandomResizedCrop
          size: 224
          scale: [0.75, 1.]
          ratio: [1., 1.]
        - name: ToTensor
        - name: Normalize
          mean: [0.48145466, 0.4578275, 0.40821073]
          std: [0.26862954, 0.26130258, 0.27577711]
      shuffle: False

solver:
  lr:
    name: CosineAnnealingDecay
    learning_rate: 0.00001
    T_max: 10
  visual_lr:
    name: CosineAnnealingDecay
    learning_rate: 0.00001
    T_max: 10
  textual_lr:
    name: CosineAnnealingDecay
    learning_rate: 0.00001
    T_max: 10

separete: True
optimizer:
  name: AdamW
  beta1: 0.9
  beta2: 0.98
  epsilon: 1e-8
  weight_decay: 0.0005

optimizer_config:
  name: CLIPOptimizerHook

log_config:
    name: LogHook
    interval: 20
