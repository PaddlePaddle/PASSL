# global configs
Global:
  task_type: Classification
  train_loop: ClassificationTrainingEpochLoop
  validate_loop: ClassificationEvaluationLoop
  checkpoint: null
  pretrained_model: swav_800ep_pretrain_adjustresnetn
  finetune: True
  output_dir: ./output/semi_0506_semi10
  device: gpu
  save_interval: 1
  max_num_latest_checkpoint: 0
  eval_during_train: True
  eval_interval: 1
  eval_unit: "epoch"
  accum_steps: 1
  epochs: 20
  print_batch_step: 50
  use_visualdl: False
  seed: 31

DistributedStrategy:
  data_parallel: True

# model architecture
Model:
  name: swav_resnet50_finetune
  backbone:
    type: swavresnet50
    output_dim: 1000

# loss function config for traing/eval process
Loss:
  Train:
    - CELoss:
        weight: 1.0
  Eval:
    - CELoss:
        weight: 1.0

Optimizer:
  name: Momentum
  momentum: 0.9
  weight_decay: 0.0
  tensor_fusion: False
  lr_decay_unit: epoch
  lr:
    name: MultiStepDecay
    learning_rate: 0.02
    milestones: [12, 16]
    gamma: 0.2
    last_epoch: -1
  param_groups:
    - name: res_model.projection_head
      lr:
        name: MultiStepDecay
        learning_rate: 5
        milestones: [12, 16]
        gamma: 0.2
        last_epoch: -1

# data loader for train and eval
DataLoader:
  Train:
    dataset:
      name: ImageFolder
      root: data/ILSVRC2012/train
      transform:
        - RandomResizedCrop:
            size: 224
        - RandomHorizontalFlip:
        - ToTensor:
        - Normalize:
            mean: [0.485, 0.456, 0.406]
            std: [0.228, 0.224, 0.225]
      samples_tag: semi_10
    sampler:
      name: DistributedBatchSampler
      batch_size: 128 # accum_steps: 1, total batchsize: 256
      drop_last: False
      shuffle: True
    loader:
      num_workers: 8
      use_shared_memory: True

  Eval:
    dataset:
      name: ImageFolder
      root: data/ILSVRC2012/val
      transform:
        - Resize:
            size: 256
        - CenterCrop:
            size: 224
        - ToTensor:
        - Normalize:
            mean: [0.485, 0.456, 0.406]
            std: [0.228, 0.224, 0.225]
    sampler:
      name: DistributedBatchSampler
      batch_size: 128
      drop_last: False
      shuffle: False
    loader:
      num_workers: 8
      use_shared_memory: True

Metric:
  Train:
    - TopkAcc:
        topk: [1, 5]
  Eval:
    - TopkAcc:
        topk: [1, 5]

Export:
  export_type: paddle
  input_shape: [None, 3, 224, 224]
